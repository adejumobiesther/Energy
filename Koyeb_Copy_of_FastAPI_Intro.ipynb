{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adejumobiesther/Energy/blob/main/Koyeb_Copy_of_FastAPI_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok openai pydantic"
      ],
      "metadata": {
        "id": "ncs4j4T6kc_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from sqlalchemy import (\n",
        "    Column, Integer, String, Text, Float, DateTime\n",
        ")\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "import datetime\n",
        "\n",
        "Base = declarative_base()\n",
        "\n",
        "class Metric(Base):\n",
        "    __tablename__ = \"metrics\"\n",
        "\n",
        "    id   = Column(Integer,  primary_key=True, autoincrement=True)\n",
        "    thread_id = Column(String(64), nullable=True)\n",
        "\n",
        "    user_text = Column(Text, nullable=False)\n",
        "    assistant_text = Column(Text, nullable=False)\n",
        "\n",
        "    out_len = Column(Integer, nullable=False)          # tokens\n",
        "    latency_ms  = Column(Float, nullable=False)\n",
        "    gen_time_ms  = Column(Float, nullable=False)\n",
        "    tps  = Column(Float, nullable=False)          # tokens/sec\n",
        "\n",
        "    energy_usage = Column(Float,nullable=False)\n",
        "    water_usage  = Column(Float,nullable=False)\n",
        "    carbon_usage = Column(Float,nullable=False)\n",
        "\n",
        "    created_at   = Column(DateTime, nullable=False,\n",
        "                             default=datetime.datetime.utcnow)\n"
      ],
      "metadata": {
        "id": "0A_IDsGhlFbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "engine = create_engine(\"postgresql://neondb_owner:npg_3jcGsgCk5rBE@ep-wild-water-afh5357n-pooler.c-2.us-west-2.aws.neon.tech/neondb?sslmode=require&channel_binding=require\",\n",
        "                       pool_pre_ping=True,   # ‚Üê ping before each use; reopen if dead\n",
        "                       pool_recycle=1800)\n",
        "Base.metadata.create_all(engine)\n",
        "\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "Session = sessionmaker(bind=engine)"
      ],
      "metadata": {
        "id": "URnN7PXCnmY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "power_profiles = {\n",
        "    \"gpt-4o-mini\": {\n",
        "        \"PGPU_kW\":     3.2,\n",
        "        \"PnonGPU_kW\":  3.3,\n",
        "        \"G\":           4,\n",
        "        \"N\":           8,\n",
        "        \"B\":           8,\n",
        "        \"D_gpu\":       [1.00, 1.10],\n",
        "        \"D_non\":       0.50,\n",
        "        \"PUE\":         1.12,\n",
        "    },\n",
        "      \"gpt-4.1\": {\n",
        "        \"PGPU_kW\":     5.6,\n",
        "        \"PnonGPU_kW\":  4.6,\n",
        "        \"G\":           8,\n",
        "        \"N\":           8,\n",
        "        \"B\":           8,\n",
        "        \"D_gpu\":       [0.45, 0.60] ,\n",
        "        \"D_non\":       0.50,\n",
        "        \"PUE\":         1.12,\n",
        "    },\n",
        "    \"gpt-4.1-nano\": {\n",
        "        \"PGPU_kW\": 5.6,\n",
        "        \"PnonGPU_kW\": 4.6,\n",
        "        \"G\": 1,\n",
        "        \"N\": 8,\n",
        "        \"B\": 8,\n",
        "        \"D_gpu\": [0.55, 0.80],\n",
        "        \"D_non\": 0.50,\n",
        "        \"PUE\": 1.12\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "import openai\n",
        "import gradio as gr\n",
        "import os\n",
        "import csv, uuid, time, argparse, textwrap\n",
        "from pathlib import Path\n",
        "import openai\n",
        "import tiktoken\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "def utilisation(G, D, N, B):\n",
        "    return (G * D) / (N * B)\n",
        "def energy_per_query(out_len, tps, latency, PGPU, Pnon, G, N, B, D_gpu, D_non, pue):\n",
        "    U_gpu  = utilisation(G, D_gpu,  N, B)\n",
        "    U_non  = utilisation(G, D_non,  N, B)\n",
        "    total_time_h  = (out_len / tps + latency) / 3600.0\n",
        "    node_power_kW = PGPU * U_gpu + Pnon * U_non\n",
        "    return total_time_h * node_power_kW * pue\n",
        "\n",
        "ENCODER = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "def token_count(txt):\n",
        "    return len(ENCODER.encode(txt))"
      ],
      "metadata": {
        "id": "PUFxrd8cnve8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import multiprocessing as mp\n",
        "import openai\n",
        "import os\n",
        "from typing import Dict, List, Optional\n",
        "from fastapi.responses import StreamingResponse\n",
        "import time, tiktoken, openai\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI(title=\"OpenAI Proxy API\")\n",
        "\n",
        "# Enable CORS for all origins\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Allow all\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# OpenAI API key input\n",
        "OPENAI_API_KEY = input(\"Enter your OpenAI API Key: \")\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "class PromptRequest(BaseModel):\n",
        "    user_message: str\n",
        "    final_prompt: str\n",
        "    extracted_history: list[dict]\n",
        "    stream: bool = True\n",
        "    threadId: str\n",
        "\n",
        "def openai_stream_generator(messages, thread_id):\n",
        "    #session = Session()\n",
        "    buffer = \"\"\n",
        "    output_tokens = 0\n",
        "    start_wall = time.perf_counter()\n",
        "    first_tok_time = None\n",
        "    model=\"gpt-4.1\"\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "          model=model,\n",
        "          messages=messages,\n",
        "          temperature=0.7,\n",
        "          max_tokens=800,\n",
        "          stream=True\n",
        "    )\n",
        "    for chunk in response:\n",
        "        delta = chunk.choices[0].delta.content or \"\"\n",
        "        if not delta:\n",
        "            continue\n",
        "        if first_tok_time is None:\n",
        "            first_tok_time = time.perf_counter()\n",
        "            print(first_tok_time)\n",
        "        buffer += delta\n",
        "        yield f\"data: {delta}\\n\\n\"\n",
        "    yield \"data: [DONE]\\n\\n\"\n",
        "    output_tokens = token_count(buffer)\n",
        "    end_wall = time.perf_counter()\n",
        "    print(end_wall)\n",
        "    latency = first_tok_time - start_wall\n",
        "    gen_time = end_wall - first_tok_time\n",
        "    tps = output_tokens / gen_time\n",
        "    print(latency)\n",
        "    print(gen_time)\n",
        "    print(tps)\n",
        "    model_profile = power_profiles.get(model)\n",
        "    if not model_profile:\n",
        "      return 0, 0, 0, 0, 0, f\"Error: Power profile not found for model: {model}\"\n",
        "\n",
        "    PGPU = model_profile[\"PGPU_kW\"]\n",
        "    Pnon = model_profile[\"PnonGPU_kW\"]\n",
        "    G = model_profile[\"G\"]\n",
        "    N = model_profile[\"N\"]\n",
        "    B = model_profile[\"B\"]\n",
        "    D_gpu_list = model_profile[\"D_gpu\"]\n",
        "    D_non = model_profile[\"D_non\"]\n",
        "    pue = model_profile[\"PUE\"]\n",
        "\n",
        "    D_gpu = sum(D_gpu_list) / len(D_gpu_list) if D_gpu_list else 0\n",
        "\n",
        "    energy_usage = energy_per_query(output_tokens, tps, latency, PGPU, Pnon, G, N, B, D_gpu, D_non, pue)\n",
        "    wue_site, wue_source = 0.3, 3.142\n",
        "    water_usage = (energy_usage/pue * wue_site + energy_usage * wue_source)\n",
        "    carbon_usage = energy_usage * 0.3528\n",
        "\n",
        "    with Session() as session:\n",
        "      # metrics\n",
        "      metric = Metric(                     # your ORM class\n",
        "            thread_id     = thread_id,\n",
        "            user_text     = messages[-1][\"content\"],\n",
        "            assistant_text= buffer,\n",
        "            out_len       = output_tokens,\n",
        "            latency_ms    = latency,\n",
        "            gen_time_ms   = gen_time,\n",
        "            tps           = tps,\n",
        "            energy_usage  = energy_usage,\n",
        "            water_usage   = water_usage,\n",
        "            carbon_usage  = carbon_usage,\n",
        "         )\n",
        "      session.add(metric)\n",
        "      session.commit()\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"message\": \"Welcome to the OpenAI Proxy API!\"}\n",
        "\n",
        "@app.post(\"/ask\")\n",
        "async def student_chat(req: PromptRequest):\n",
        "    msgs = (\n",
        "        [{\"role\":\"system\",\"content\":req.final_prompt}]\n",
        "        + req.extracted_history\n",
        "        + [{\"role\":\"user\",\"content\":req.user_message}]\n",
        "    )\n",
        "    gen = openai_stream_generator(msgs, thread_id=req.threadId)\n",
        "    return StreamingResponse(gen, media_type=\"text/event-stream\")"
      ],
      "metadata": {
        "id": "4LTGZv-7n4IP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}