# -*- coding: utf-8 -*-
"""Koyeb Copy of FastAPI Intro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YUq_YUbAA9ZQECA43ZkXU4xgwkmX5DcE
"""

import datetime
from sqlalchemy import (
    Column, Integer, String, Text, Float, DateTime
)
from sqlalchemy.ext.declarative import declarative_base
import datetime
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
import openai
import os
import csv, uuid, time, argparse, textwrap
from pathlib import Path
import openai
import tiktoken
from huggingface_hub import InferenceClient


Base = declarative_base()

class Metric(Base):
    __tablename__ = "metrics"

    id   = Column(Integer,  primary_key=True, autoincrement=True)
    thread_id = Column(String(64), nullable=True)

    user_text = Column(Text, nullable=False)
    assistant_text = Column(Text, nullable=False)

    out_len = Column(Integer, nullable=False)          # tokens
    latency_ms  = Column(Float, nullable=False)
    gen_time_ms  = Column(Float, nullable=False)
    tps  = Column(Float, nullable=False)          # tokens/sec

    energy_usage = Column(Float,nullable=False)
    water_usage  = Column(Float,nullable=False)
    carbon_usage = Column(Float,nullable=False)

    created_at   = Column(DateTime, nullable=False,
                             default=datetime.datetime.utcnow)
    model         = Column(String) 

engine = create_engine("postgresql://neondb_owner:npg_3jcGsgCk5rBE@ep-wild-water-afh5357n-pooler.c-2.us-west-2.aws.neon.tech/neondb?sslmode=require&channel_binding=require",
                       pool_pre_ping=True,   # ← ping before each use; reopen if dead
                       pool_recycle=1800)
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

power_profiles = {
    "gpt-4o-mini": {
        "PGPU_kW":     3.2,
        "PnonGPU_kW":  3.3,
        "G":           4,
        "N":           8,
        "B":           8,
        "D_gpu":       [1.00, 1.10],
        "D_non":       0.50,
        "PUE":         1.12,
    },
      "gpt-4.1": {
        "PGPU_kW":     5.6,
        "PnonGPU_kW":  4.6,
        "G":           8,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.45, 0.60] ,
        "D_non":       0.50,
        "PUE":         1.12,
    },
    "gpt-4.1-nano": {
        "PGPU_kW": 5.6,
        "PnonGPU_kW": 4.6,
        "G": 1,
        "N": 8,
        "B": 8,
        "D_gpu": [0.55, 0.80],
        "D_non": 0.50,
        "PUE": 1.12
    },
    "o3-mini": {
      "PGPU_kW": 5.6,
      "PnonGPU_kW": 4.6,
      "G": 4,
      "N": 8,
      "B": 8,
      "D_gpu": [1.00, 1.10],
      "D_non": 0.50,
      "PUE": 1.12
    },
    "o3": {
      "PGPU_kW": 5.6,
      "PnonGPU_kW": 4.6,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [0.45, 0.60],
      "D_non": 0.50,
      "PUE": 1.12
    },
    "o1": {
      "PGPU_kW": 5.6,
      "PnonGPU_kW": 4.6,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [0.45, 0.60],
      "D_non": 0.50,
      "PUE": 1.12
    },
    "o1-mini": {
      "PGPU_kW": 5.6,
      "PnonGPU_kW": 4.6,
      "G": 4,
      "N": 8,
      "B": 8,
      "D_gpu": [0.50, 0.70],
      "D_non": 0.50,
      "PUE": 1.12
    },
    "gpt-4o": {
      "PGPU_kW": 5.6,
      "PnonGPU_kW": 4.6,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [0.45, 0.60],
      "D_non": 0.50,
      "PUE": 1.12
    },
    "gpt-4-turbo": {
      "PGPU_kW": 3.2,
      "PnonGPU_kW": 3.3,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [1.00, 1.20],
      "D_non": 0.50,
      "PUE": 1.12
    },
    "gpt-4": {
      "PGPU_kW": 3.2,
      "PnonGPU_kW": 3.3,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [1.00, 1.20],
      "D_non": 0.50,
      "PUE": 1.12
    },
    "meta-llama/Meta-Llama-3-8B-Instruct": {
       "PGPU_kW":     5.6,
        "PnonGPU_kW":  4.6,
        "G":           1,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.5, 0.8] ,
        "D_non":       0.50,
        "PUE":         1.12,
    },
    "meta-llama/Meta-Llama-3-70B-Instruct": {
        "PGPU_kW":     5.6,
        "PnonGPU_kW":  4.6,
        "G":           4,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.5, 0.7] ,
        "D_non":       0.50,
        "PUE":         1.12,
    },
    "meta-llama/Llama-3.1-70B-Instruct": {
      "PGPU_kW":     5.6,
      "PnonGPU_kW":  4.6,
        "G":           4,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.5, 0.7] ,
        "D_non":       0.50,
        "PUE":         1.12,
    },
    "meta-llama/Llama-3.1-405B-Instruct": {
        "PGPU_kW":     5.6,
        "PnonGPU_kW":  4.6,
        "G":           8,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.45, 0.60],
        "D_non":       0.50,
        "PUE":         1.12,
    },
    "meta-llama/Llama-3.2-11B-Vision-Instruct": {
        "PGPU_kW":     5.6,
        "PnonGPU_kW":  4.6,
        "G":           1,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.5, 0.8] ,
        "D_non":       0.50,
        "PUE":         1.12,
    },
     "meta-llama/Llama-3.3-70B-Instruct": {
        "PGPU_kW":     5.6,
        "PnonGPU_kW":  4.6,
        "G":           4,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.5, 0.7] ,
        "D_non":       0.50,
        "PUE":         1.12,
    },
    "claude-3-7-sonnet-latest": {
      "PGPU_kW":     5.6,
      "PnonGPU_kW":  4.6,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [0.45, 0.60],
      "D_non": 0.50,
      "PUE": 1.14
    },
    "claude-3-5-sonnet-20240620": {
      "PGPU_kW":     5.6,
      "PnonGPU_kW":  4.6,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [0.45, 0.60],
      "D_non": 0.50,
      "PUE": 1.14
    },
    "claude-3-5-haiku-latest": {
     "PGPU_kW":     5.6,
     "PnonGPU_kW":  4.6,
      "G": 4,
      "N": 8,
      "B": 8,
      "D_gpu": [1.00, 1.10],
      "D_non": 0.50,
      "PUE": 1.14
    },
  }


def utilisation(G, D, N, B):
    return (G * D) / (N * B)
def energy_per_query(out_len, tps, latency, PGPU, Pnon, G, N, B, D_gpu, D_non, pue):
    U_gpu  = utilisation(G, D_gpu,  N, B)
    U_non  = utilisation(G, D_non,  N, B)
    total_time_h  = (out_len / tps + latency) / 3600.0
    node_power_kW = PGPU * U_gpu + Pnon * U_non
    return total_time_h * node_power_kW * pue

ENCODER = tiktoken.get_encoding('cl100k_base')

def token_count(txt):
    return len(ENCODER.encode(txt))


from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import multiprocessing as mp
import openai
import os
from typing import Dict, List, Optional
from fastapi.responses import StreamingResponse
import time, tiktoken, openai

# Initialize FastAPI
app = FastAPI(title="OpenAI Proxy API")

# Enable CORS for all origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY environment variable is not set")
openai.api_key = OPENAI_API_KEY

class PromptRequest(BaseModel):
    user_message: str
    final_prompt: str
    extracted_history: list[dict]
    stream: bool = True
    threadId: str
    model:str

import os, time, itertools
import openai, anthropic
from huggingface_hub import InferenceClient
from fastapi import HTTPException

# --------------------------------------------------------------------------
#  Model groups
# --------------------------------------------------------------------------
OPENAI_MODELS = {
    'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4',
    'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano',
    'o4-mini', 'o3', 'o3-mini', 'o3-mini-high',
    'o1', 'o1-mini',
}
HF_MODELS = {
    "meta-llama/Meta-Llama-3-8B-Instruct",
    "meta-llama/Meta-Llama-3-70B-Instruct",
    "meta-llama/Llama-3.1-70B-Instruct",
    "meta-llama/Llama-3.1-405B-Instruct",
    "meta-llama/Llama-3.2-11B-Vision-Instruct",
    "meta-llama/Llama-3.3-70B-Instruct",
}
ANTHROPIC_MODELS = {
    "claude-3-7-sonnet-latest",
    "claude-3-5-sonnet-20240620",
    "claude-3-5-haiku-latest",
}

power_profiles = {
    "gpt-4o-mini": {
        "PGPU_kW":     3.2,
        "PnonGPU_kW":  3.3,
        "G":           4,
        "N":           8,
        "B":           8,
        "D_gpu":       [1.00, 1.10],
        "D_non":       0.50,
        "PUE":         1.12,
        "WUE_site": 0.30, "WUE_source": 3.142,
        "CIF": 0.3528,
    },
      "gpt-4.1": {
        "PGPU_kW":     5.6,
        "PnonGPU_kW":  4.6,
        "G":           8,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.45, 0.60] ,
        "D_non":       0.50,
        "PUE":         1.12,
        "WUE_site": 0.30, "WUE_source": 3.142,
        "CIF": 0.3528,
    },
    "gpt-4.1-nano": {
        "PGPU_kW": 5.6,
        "PnonGPU_kW": 4.6,
        "G": 1,
        "N": 8,
        "B": 8,
        "D_gpu": [0.55, 0.80],
        "D_non": 0.50,
        "PUE": 1.12,
        "WUE_site": 0.30, "WUE_source": 3.142,
        "CIF": 0.3528,
    },
    "o3-mini": {
      "PGPU_kW": 5.6,
      "PnonGPU_kW": 4.6,
      "G": 4,
      "N": 8,
      "B": 8,
      "D_gpu": [1.00, 1.10],
      "D_non": 0.50,
      "PUE": 1.12,
      "WUE_site": 0.30, "WUE_source": 3.142,
      "CIF": 0.3528,
    },
    "o3": {
      "PGPU_kW": 5.6,
      "PnonGPU_kW": 4.6,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [0.45, 0.60],
      "D_non": 0.50,
      "PUE": 1.12,
      "WUE_site": 0.30, "WUE_source": 3.142,
      "CIF": 0.3528,
    },
    "o1": {
      "PGPU_kW": 5.6,
      "PnonGPU_kW": 4.6,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [0.45, 0.60],
      "D_non": 0.50,
      "PUE": 1.12,
      "WUE_site": 0.30, "WUE_source": 3.142,
      "CIF": 0.3528,
    },
    "o1-mini": {
      "PGPU_kW": 5.6,
      "PnonGPU_kW": 4.6,
      "G": 4,
      "N": 8,
      "B": 8,
      "D_gpu": [0.50, 0.70],
      "D_non": 0.50,
      "PUE": 1.12,
      "WUE_site": 0.30, "WUE_source": 3.142,
      "CIF": 0.3528,
    },
    "gpt-4o": {
      "PGPU_kW": 5.6,
      "PnonGPU_kW": 4.6,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [0.45, 0.60],
      "D_non": 0.50,
      "PUE": 1.12,
      "WUE_site": 0.30, "WUE_source": 3.142,
      "CIF": 0.3528,
    },
    "gpt-4-turbo": {
      "PGPU_kW": 3.2,
      "PnonGPU_kW": 3.3,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [1.00, 1.20],
      "D_non": 0.50,
      "PUE": 1.12,
      "WUE_site": 0.30, "WUE_source": 3.142,
      "CIF": 0.3528,
    },
    "gpt-4": {
      "PGPU_kW": 3.2,
      "PnonGPU_kW": 3.3,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [1.00, 1.20],
      "D_non": 0.50,
      "PUE": 1.12,
      "WUE_site": 0.30, "WUE_source": 3.142,
      "CIF": 0.3528,
    },
    "meta-llama/Meta-Llama-3-8B-Instruct": {
       "PGPU_kW":     5.6,
        "PnonGPU_kW":  4.6,
        "G":           1,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.5, 0.8] ,
        "D_non":       0.50,
        "PUE":         1.12,
        "WUE_site": 0.18, "WUE_source": 3.142,
        "CIF": 0.385,
    },
    "meta-llama/Meta-Llama-3-70B-Instruct": {
        "PGPU_kW":     5.6,
        "PnonGPU_kW":  4.6,
        "G":           4,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.5, 0.7] ,
        "D_non":       0.50,
        "PUE":         1.12,
        "WUE_site": 0.18, "WUE_source": 3.142,
        "CIF": 0.385,
    },
    "meta-llama/Llama-3.1-70B-Instruct": {
      "PGPU_kW":     5.6,
      "PnonGPU_kW":  4.6,
        "G":           4,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.5, 0.7] ,
        "D_non":       0.50,
        "PUE":         1.12,
        "WUE_site": 0.18, "WUE_source": 3.142,
        "CIF": 0.385,
    },
    "meta-llama/Llama-3.1-405B-Instruct": {
        "PGPU_kW":     5.6,
        "PnonGPU_kW":  4.6,
        "G":           8,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.45, 0.60],
        "D_non":       0.50,
        "PUE":         1.12,
        "WUE_site": 0.18, "WUE_source": 3.142,
        "CIF": 0.385,
    },
    "meta-llama/Llama-3.2-11B-Vision-Instruct": {
        "PGPU_kW":     5.6,
        "PnonGPU_kW":  4.6,
        "G":           1,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.5, 0.8] ,
        "D_non":       0.50,
        "PUE":         1.12,
        "WUE_site": 0.18, "WUE_source": 3.142,
        "CIF": 0.385,
    },
     "meta-llama/Llama-3.3-70B-Instruct": {
        "PGPU_kW":     5.6,
        "PnonGPU_kW":  4.6,
        "G":           4,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.5, 0.7] ,
        "D_non":       0.50,
        "PUE":         1.12,
        "WUE_site": 0.18, "WUE_source": 3.142,
        "CIF": 0.385,
    },
    "claude-3-7-sonnet-latest": {
      "PGPU_kW":     5.6,
      "PnonGPU_kW":  4.6,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [0.45, 0.60],
      "D_non": 0.50,
      "PUE": 1.14,
      "WUE_site": 0.18, "WUE_source": 3.142,
      "CIF": 0.385,
    },
    "claude-3-5-sonnet-20240620": {
      "PGPU_kW":     5.6,
      "PnonGPU_kW":  4.6,
      "G": 8,
      "N": 8,
      "B": 8,
      "D_gpu": [0.45, 0.60],
      "D_non": 0.50,
      "PUE": 1.14,
      "WUE_site": 0.18, "WUE_source": 3.142,
      "CIF": 0.385,
    },
    "claude-3-5-haiku-latest": {
     "PGPU_kW":     5.6,
     "PnonGPU_kW":  4.6,
      "G": 4,
      "N": 8,
      "B": 8,
      "D_gpu": [1.00, 1.10],
      "D_non": 0.50,
      "PUE": 1.14,
      "WUE_site": 0.18, "WUE_source": 3.142,
      "CIF": 0.385,
    },
  }

import os, time
import openai, anthropic
from huggingface_hub import InferenceClient
from fastapi import HTTPException

# ------------------------------------------------------------------------
OPENAI_MODELS = {
    'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4',
    'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano',
    'o4-mini', 'o3', 'o3-mini', 'o3-mini-high', 'o1', 'o1-mini',
}
HF_MODELS = {
    "meta-llama/Meta-Llama-3-8B-Instruct",
    "meta-llama/Meta-Llama-3-70B-Instruct",
    "meta-llama/Llama-3.1-70B-Instruct",
    "meta-llama/Llama-3.1-405B-Instruct",
    "meta-llama/Llama-3.2-11B-Vision-Instruct",
    "meta-llama/Llama-3.3-70B-Instruct",
}
ANTHROPIC_MODELS = {
    "claude-3-7-sonnet-latest",
    "claude-3-5-sonnet-20240620",
    "claude-3-5-haiku-latest",
}

# ------------------------------------------------------------------------
def llm_stream_generator(messages, model_val, thread_id):
    """
    Streams tokens for OpenAI / HF‑Llama / Claude models and performs the SAME
    metric + energy / water / carbon calculations you already had.
    """
    buffer            = ""                         # ← string, like before
    start_wall        = time.perf_counter()
    first_tok_time    = None
    model             = model_val                  # keep identical name

    # -- choose backend -----------------------------------------------------
    if   model_val in OPENAI_MODELS:    provider = "openai"
    elif model_val in HF_MODELS:        provider = "hf"
    elif model_val in ANTHROPIC_MODELS: provider = "anthropic"
    else:
        raise HTTPException(400, f"Unsupported model '{model_val}'")

    # -- create iterator and unwrap function -------------------------------
    if provider == "openai":
        iterator = openai.chat.completions.create(
            model=model_val,
            messages=messages,
            temperature=0.7,
            stream=True,
        )
        unwrap = lambda chunk: chunk.choices[0].delta.content or ""

    elif provider == "hf":
        prompt   = messages[-1]["content"]
        client   = InferenceClient(model=model_val, token=os.getenv("HF_TOKEN"))
        iterator = client.chat_completion(
            [{"role": "user", "content": prompt}], stream=True)
        unwrap = lambda chunk: chunk.choices[0].delta.content or ""

    else:  # anthropic
        prompt  = messages[-1]["content"]
        client  = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        iterator = client.messages.create(
            model=model_val,
            messages=[{"role": "user", "content": prompt}],
            stream=True,
        )
        def unwrap(chunk):
            return (chunk.delta.text
                    if getattr(chunk, "type", "") == "content_block_delta"
                    else "")

    # -- streaming loop -----------------------------------------------------
    for raw in iterator:
        delta = unwrap(raw)
        if not delta:
            continue
        if first_tok_time is None:
            first_tok_time = time.perf_counter()
        buffer += delta                    # SAME mutation style
        yield f"data: {delta}\n\n"

    # end of content
    end_wall = time.perf_counter()
    yield "data: [DONE]\n\n"

    # ----------------------------------------------------------------------
    #  *** EXACT SAME CALCULATION BLOCK YOU PROVIDED ***
    # ----------------------------------------------------------------------
    output_tokens = token_count(buffer)
    latency       = first_tok_time - start_wall
    gen_time      = end_wall - first_tok_time
    tps           = output_tokens / gen_time

    model_profile = power_profiles.get(model_val)
    if not model_profile:
        return 0, 0, 0, 0, 0, f"Error: Power profile not found for model: {model}"

    PGPU         = model_profile["PGPU_kW"]
    Pnon         = model_profile["PnonGPU_kW"]
    G            = model_profile["G"]
    N            = model_profile["N"]
    B            = model_profile["B"]
    D_gpu_list   = model_profile["D_gpu"]
    D_non        = model_profile["D_non"]
    pue          = model_profile["PUE"]
    D_gpu        = sum(D_gpu_list) / len(D_gpu_list) if D_gpu_list else 0

    energy_usage = energy_per_query(
        output_tokens, tps, latency,
        PGPU, Pnon, G, N, B, D_gpu, D_non, pue,
    )

    # ------------- CONSTANTS REMAIN UNCHANGED -----------------------------
    wue_site   = model_profile.get("WUE_site")
    wue_source = model_profile.get("WUE_source")
    cif        = model_profile.get("CIF")

    water_usage  = energy_usage / pue * wue_site + energy_usage * wue_source
    carbon_usage = energy_usage * cif

    # -- ORM write, unchanged ----------------------------------------------
    with Session() as session:
        metric = Metric(
            thread_id      = thread_id,
            user_text      = messages[-1]["content"],
            assistant_text = buffer,
            out_len        = output_tokens,
            latency_ms     = latency,
            gen_time_ms    = gen_time,
            tps            = tps,
            energy_usage   = energy_usage,
            water_usage    = water_usage,
            carbon_usage   = carbon_usage,
            model = model_val
        )
        session.add(metric)
        session.commit()

@app.get("/")
def root():
    return {"message": "Welcome to the OpenAI Proxy API!"}

@app.post("/ask")
async def student_chat(req: PromptRequest):
    msgs = (
        [{"role":"system","content":req.final_prompt}]
        + req.extracted_history
        + [{"role":"user","content":req.user_message}]
    )
    gen = llm_stream_generator(msgs, req.model, thread_id=req.threadId)
    return StreamingResponse(gen, media_type="text/event-stream")
