# -*- coding: utf-8 -*-
"""Koyeb Copy of FastAPI Intro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YUq_YUbAA9ZQECA43ZkXU4xgwkmX5DcE
"""

import datetime
from sqlalchemy import (
    Column, Integer, String, Text, Float, DateTime
)
from sqlalchemy.ext.declarative import declarative_base
import datetime
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
import openai
import os
import csv, uuid, time, argparse, textwrap
from pathlib import Path
import openai
import tiktoken
from huggingface_hub import InferenceClient


Base = declarative_base()

class Metric(Base):
    __tablename__ = "metrics"

    id   = Column(Integer,  primary_key=True, autoincrement=True)
    thread_id = Column(String(64), nullable=True)

    user_text = Column(Text, nullable=False)
    assistant_text = Column(Text, nullable=False)

    out_len = Column(Integer, nullable=False)          # tokens
    latency_ms  = Column(Float, nullable=False)
    gen_time_ms  = Column(Float, nullable=False)
    tps  = Column(Float, nullable=False)          # tokens/sec

    energy_usage = Column(Float,nullable=False)
    water_usage  = Column(Float,nullable=False)
    carbon_usage = Column(Float,nullable=False)

    created_at   = Column(DateTime, nullable=False,
                             default=datetime.datetime.utcnow)

engine = create_engine("postgresql://neondb_owner:npg_3jcGsgCk5rBE@ep-wild-water-afh5357n-pooler.c-2.us-west-2.aws.neon.tech/neondb?sslmode=require&channel_binding=require",
                       pool_pre_ping=True,   # ‚Üê ping before each use; reopen if dead
                       pool_recycle=1800)
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

power_profiles = {
    "gpt-4o-mini": {
        "PGPU_kW":     3.2,
        "PnonGPU_kW":  3.3,
        "G":           4,
        "N":           8,
        "B":           8,
        "D_gpu":       [1.00, 1.10],
        "D_non":       0.50,
        "PUE":         1.12,
    },
      "gpt-4.1": {
        "PGPU_kW":     5.6,
        "PnonGPU_kW":  4.6,
        "G":           8,
        "N":           8,
        "B":           8,
        "D_gpu":       [0.45, 0.60] ,
        "D_non":       0.50,
        "PUE":         1.12,
    },
    "gpt-4.1-nano": {
        "PGPU_kW": 5.6,
        "PnonGPU_kW": 4.6,
        "G": 1,
        "N": 8,
        "B": 8,
        "D_gpu": [0.55, 0.80],
        "D_non": 0.50,
        "PUE": 1.12
    }
}


def utilisation(G, D, N, B):
    return (G * D) / (N * B)
def energy_per_query(out_len, tps, latency, PGPU, Pnon, G, N, B, D_gpu, D_non, pue):
    U_gpu  = utilisation(G, D_gpu,  N, B)
    U_non  = utilisation(G, D_non,  N, B)
    total_time_h  = (out_len / tps + latency) / 3600.0
    node_power_kW = PGPU * U_gpu + Pnon * U_non
    return total_time_h * node_power_kW * pue

ENCODER = tiktoken.get_encoding('cl100k_base')

def token_count(txt):
    return len(ENCODER.encode(txt))


from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import multiprocessing as mp
import openai
import os
from typing import Dict, List, Optional
from fastapi.responses import StreamingResponse
import time, tiktoken, openai

# Initialize FastAPI
app = FastAPI(title="OpenAI Proxy API")

# Enable CORS for all origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY environment variable is not set")
openai.api_key = OPENAI_API_KEY

class PromptRequest(BaseModel):
    user_message: str
    final_prompt: str
    extracted_history: list[dict]
    stream: bool = True
    threadId: str
    model:str

def openai_stream_generator(messages, model_val, thread_id):
    try:
        #session = Session()
        buffer = ""
        output_tokens = 0
        start_wall = time.perf_counter()
        first_tok_time = None
        model=model_val

        response = openai.chat.completions.create(
          model=model,
          messages=messages,
          temperature=0.7,
          max_tokens=800,
          stream=True
        )
        for chunk in response:
            delta = chunk.choices[0].delta.content or ""
            if not delta:
                continue
            if first_tok_time is None:
                first_tok_time = time.perf_counter()
                print(first_tok_time)
            buffer += delta
            yield f"data: {delta}\n\n"
        yield "data: [DONE]\n\n"
        end_wall = time.perf_counter()
    finally:
        output_tokens = token_count(buffer)
        latency = first_tok_time - start_wall
        gen_time = end_wall - first_tok_time
        tps = output_tokens / gen_time
        model_profile = power_profiles.get(model)
        if not model_profile:
            return 0, 0, 0, 0, 0, f"Error: Power profile not found for model: {model}"

        PGPU = model_profile["PGPU_kW"]
        Pnon = model_profile["PnonGPU_kW"]
        G = model_profile["G"]
        N = model_profile["N"]
        B = model_profile["B"]
        D_gpu_list = model_profile["D_gpu"]
        D_non = model_profile["D_non"]
        pue = model_profile["PUE"]

        D_gpu = sum(D_gpu_list) / len(D_gpu_list) if D_gpu_list else 0

        energy_usage = energy_per_query(output_tokens, tps, latency, PGPU, Pnon, G, N, B, D_gpu, D_non, pue)
        wue_site, wue_source = 0.3, 3.142
        water_usage = (energy_usage/pue * wue_site + energy_usage * wue_source)
        carbon_usage = energy_usage * 0.3528

        with Session() as session:
            # metrics
            metric = Metric(                     # your ORM class
                thread_id     = thread_id,
                user_text     = messages[-1]["content"],
                assistant_text= buffer,
                out_len       = output_tokens,
                latency_ms    = latency,
                gen_time_ms   = gen_time,
                tps           = tps,
                energy_usage  = energy_usage,
                water_usage   = water_usage,
                carbon_usage  = carbon_usage,
             )
            session.add(metric)
            session.commit()

@app.get("/")
def root():
    return {"message": "Welcome to the OpenAI Proxy API!"}

@app.post("/ask")
async def student_chat(req: PromptRequest):
    msgs = (
        [{"role":"system","content":req.final_prompt}]
        + req.extracted_history
        + [{"role":"user","content":req.user_message}]
    )
    gen = openai_stream_generator(msgs, req.model, thread_id=req.threadId)
    return StreamingResponse(gen, media_type="text/event-stream")
